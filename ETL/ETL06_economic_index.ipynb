{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 06 Extract economic index from alphavantage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql import functions as f\r\n",
        "import requests\r\n",
        "import pandas as pd \r\n",
        "from datetime import datetime, timedelta\r\n",
        "from notebookutils import mssparkutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "# get data from api \n",
        "def get_GDP_PC(apikey):\n",
        "    url = 'https://www.alphavantage.co/query?function=REAL_GDP_PER_CAPITA&apikey={0}'.format(apikey)\n",
        "    r = requests.get(url)\n",
        "    data = r.json()\n",
        "    return data\n",
        "\n",
        "def get_FDI(apikey):\n",
        "    url = 'https://www.alphavantage.co/query?function=FEDERAL_FUNDS_RATE&interval=monthly&apikey={0}'.format(apikey)\n",
        "    r = requests.get(url)\n",
        "    data = r.json()\n",
        "    return data\n",
        "\n",
        "def get_CPI(apikey):\n",
        "    url = 'https://www.alphavantage.co/query?function=CPI&interval=monthly&apikey={0}'.format(apikey)\n",
        "    r = requests.get(url)\n",
        "    data = r.json()\n",
        "    return data\n",
        "\n",
        "def get_INF(apikey):\n",
        "    url = 'https://www.alphavantage.co/query?function=INFLATION&interval=monthly&apikey={0}'.format(apikey)\n",
        "    r = requests.get(url)\n",
        "    data = r.json()\n",
        "    return data\n",
        "\n",
        "def get_UNEM(apikey):\n",
        "    url = 'https://www.alphavantage.co/query?function=UNEMPLOYMENT&apikey={0}'.format(apikey)\n",
        "    r = requests.get(url)\n",
        "    data = r.json()\n",
        "    return data    \n",
        "\n",
        "def transform_spdf(data):\n",
        "    pdf = pd.DataFrame(data)\n",
        "    Spdf = spark.createDataFrame(pdf[['name','data']])\n",
        "    Spdf = (\n",
        "    Spdf.select('name','data')\n",
        "    .withColumn('date',Spdf.data.getItem(\"date\"))\n",
        "    .withColumn('value',Spdf.data.getItem('value'))\n",
        "    .select(\"name\",'date','value')\n",
        "    ) \n",
        "    return Spdf "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # get apikey from key-vault \n",
        "    apikey = mssparkutils.credentials.getSecret('synapse-key12345','alphavantage-key')\n",
        "\n",
        "    # get data from api\n",
        "    GDP = get_GDP_PC(apikey)\n",
        "    FDI = get_FDI(apikey)\n",
        "    CPI = get_CPI(apikey)\n",
        "    INF = get_INF(apikey)\n",
        "    UNEM = get_UNEM(apikey)\n",
        "    \n",
        "    GDP_SP = transform_spdf(GDP)\n",
        "    GDP_FDI = transform_spdf(FDI)\n",
        "    GDP_CPI = transform_spdf(CPI)\n",
        "    GDP_INF = transform_spdf(INF)\n",
        "    GDP_UNEM = transform_spdf(UNEM)\n",
        "\n",
        "    all_= GDP_SP.union(GDP_FDI).union(GDP_CPI).union(GDP_INF).union(GDP_UNEM)\n",
        "        \n",
        "    print('load data into filepath')\n",
        "    \n",
        "    datalake_nm = 'datalake'+mssparkutils.env.getWorkspaceName()[7:] # get datalake name \n",
        "    file_path ='abfss://files@{0}.dfs.core.windows.net/synapse/workspaces/data/economic_index'.format(datalake_nm) \n",
        "    all_.write.parquet(file_path,mode='overwrite')"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}