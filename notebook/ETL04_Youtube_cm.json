{
	"name": "ETL04_Youtube_cm",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark001",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "8abd9693-5f10-4e83-a5ff-b5dd5abab968"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ac86b402-22ae-4573-b51d-10b49ac53875/resourceGroups/dp000-2gfrzms/providers/Microsoft.Synapse/workspaces/synapse2gfrzms/bigDataPools/spark001",
				"name": "spark001",
				"type": "Spark",
				"endpoint": "https://synapse2gfrzms.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark001",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"id": "jF_ENh3atCvf"
				},
				"source": [
					"### Imports"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"!pip3 install google-api-python-client\n",
					"!pip install clean-text"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"from googleapiclient.discovery import build\n",
					"import googleapiclient.errors\n",
					"from datetime import datetime, timezone, timedelta\n",
					"import pandas as pd"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"# create connection\n",
					"def set_up_youtube(api_service_name,api_version,DEVELOPER_KEY):\n",
					"    youtube = googleapiclient.discovery.build(api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
					"    return youtube \n",
					"    \n",
					"# serch for video list \n",
					"def get_video_object(youtube,query,publishedAf,order_type):\n",
					"    \n",
					"    # request for video based on your query \n",
					"    search_response = youtube.search().list(\n",
					"        q=query,\n",
					"        type='video',\n",
					"        part='id,snippet',\n",
					"        maxResults=30,\n",
					"        publishedAfter= publishedAf,\n",
					"        order=order_type\n",
					"    ).execute()\n",
					"\n",
					"    #get video object from the response \n",
					"    videos = []\n",
					"    for search_result in search_response.get('items', []):\n",
					"        video = {\n",
					"                'title': search_result['snippet']['title'],\n",
					"                'id': search_result['id']['videoId'],\n",
					"                'url': f\"https://www.youtube.com/watch?v={search_result['id']['videoId']}\",\n",
					"                'video_id': search_result['id']['videoId'],\n",
					"                'CreatedAt':search_result['snippet']['publishedAt']\n",
					"            }\n",
					"\n",
					"        videos.append(video)\n",
					"    return videos\n",
					"\n",
					"# Function to get all comments (including replies) for a single video\n",
					"\n",
					"def get_comments_for_video(youtube, video_id):\n",
					"    all_comments = []\n",
					"    next_page_token = None\n",
					"\n",
					"    while True:\n",
					"        try:\n",
					"            comment_request = youtube.commentThreads().list(\n",
					"                part=\"snippet\",\n",
					"                videoId=video_id,\n",
					"                pageToken=next_page_token,\n",
					"                textFormat=\"plainText\",\n",
					"                maxResults=100\n",
					"            )\n",
					"            comment_response = comment_request.execute()\n",
					"\n",
					"            for item in comment_response['items']:\n",
					"                top_comment = item['snippet']['topLevelComment']['snippet']\n",
					"                all_comments.append({\n",
					"                    'Timestamp': top_comment['publishedAt'],\n",
					"                    'Username': top_comment['authorDisplayName'],\n",
					"                    'VideoID': video_id,\n",
					"                    'Comment': top_comment['textDisplay'],\n",
					"                    'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt']\n",
					"                })\n",
					"\n",
					"            next_page_token = comment_response.get('nextPageToken')\n",
					"            if not next_page_token:\n",
					"                break\n",
					"\n",
					"        except googleapiclient.errors.HttpError as e:\n",
					"            error_content = e.content.decode(\"utf-8\")\n",
					"            if 'commentsDisabled' in error_content or 'disabled comments' in error_content:\n",
					"                # Ignore videos with disabled comments\n",
					"                print(f\"Comments are disabled for the video with ID {video_id}. Ignoring this video.\")\n",
					"                break  # Exit the loop\n",
					"            else:\n",
					"                print(f\"An error occurred: {e}\")\n",
					"                break  # Exit the loop\n",
					"    \n",
					"    return all_comments"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"if __name__ == '__main__':\n",
					"    api_service_name = \"youtube\"\n",
					"    api_version = \"v3\"\n",
					"    DEVELOPER_KEY= 'AIzaSyBX_jEiRD49Pp-7r3nm1YOz3p-S0cUnPRs'\n",
					"    \n",
					"    youtube = set_up_youtube(api_service_name,api_version,DEVELOPER_KEY)\n",
					"    \n",
					"    bf_30 = datetime.now(timezone.utc)-timedelta(days=30)\n",
					"    iso_format_with_z = bf_30 .isoformat().replace('+00:00', 'Z')\n",
					"    symbol_list=['NVDA','AMD','INTC','QCOM','GOOG','MSFT','AMZN','AAPL']\n",
					"    \n",
					"    all_stocks_cm = []\n",
					"    for symbol in symbol_list:\n",
					"        video_list = get_video_object(youtube,f'{symbol} stock',iso_format_with_z,order_type='viewCount')\n",
					"        video_ids= [i['video_id'] for i in video_list]\n",
					"        \n",
					"        all_comments=[]\n",
					"        for video_id in video_ids:\n",
					"            video_comments = get_comments_for_video(youtube, video_id)\n",
					"            all_comments.extend(video_comments)\n",
					"            \n",
					"        comments_df = pd.DataFrame(all_comments)\n",
					"        comments_df['resource'] = 'Youtube'\n",
					"        comments_df['symbol'] = symbol\n",
					"        all_stocks_cm.append(comments_df)\n",
					"        \n",
					"    df = pd.concat(all_stocks_cm)\n",
					"    df['collectedAt'] = datetime.now().date()\n",
					"\n",
					"    spark_df = spark.createDataFrame(df)\n",
					"    file_path ='abfss://files@datalake2gfrzms.dfs.core.windows.net/synapse/workspaces/synapse2gfrzms/warehouse/Youtube_cms'\n",
					"\n",
					"    spark_df.write.parquet(file_path,partitionBy='collectedAt',mode='overwrite')"
				],
				"execution_count": 6
			}
		]
	}
}