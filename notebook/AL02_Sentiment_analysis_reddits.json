{
	"name": "AL02_Sentiment_analysis_reddits",
	"properties": {
		"folder": {
			"name": "Analysis"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "038afc35-5a34-4992-ac9d-5fd66fd470df"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Sentiment analysis on reddit comments"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"!pip install vaderSentiment langdetect emoji nltk vaderSentiment"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf,monotonically_increasing_id\r\n",
					"from pyspark.sql.types import StringType, FloatType\r\n",
					"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\r\n",
					"from pyspark.sql import functions as f  \r\n",
					"import requests\r\n",
					"import pandas as pd \r\n",
					"import datetime\r\n",
					"import emoji\r\n",
					"import nltk\r\n",
					"from langdetect import detect\r\n",
					"from langdetect.lang_detect_exception import LangDetectException\r\n",
					"from nltk.corpus import stopwords\r\n",
					"from nltk.tokenize import word_tokenize\r\n",
					"import string\r\n",
					"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\r\n",
					"from notebookutils import mssparkutils"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"nltk.download('punkt')\r\n",
					"nltk.download('stopwords')"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# read file from datalake\r\n",
					"datalake_nm = 'datalake'+mssparkutils.env.getWorkspaceName()[7:] # get datalake name \r\n",
					"file_path ='abfss://files@{0}.dfs.core.windows.net/synapse/workspaces/data/reddit_cms'.format(datalake_nm)\r\n",
					"reddit_df = spark.read.parquet(file_path )"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 1. Data preprocessing"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# remove non english word\r\n",
					"def is_english(text):\r\n",
					"    try:\r\n",
					"        return detect(text) == 'en'\r\n",
					"    except LangDetectException:\r\n",
					"        return False\r\n",
					"    \r\n",
					"# change text into lowercase\r\n",
					"def to_lowercase(text):\r\n",
					"    return text.lower()\r\n",
					"\r\n",
					"# remove punctuation\r\n",
					"def rm_punctuation(text):\r\n",
					"    return text.translate(str.maketrans('', '', string.punctuation))\r\n",
					"\r\n",
					"def tokenise(text):\r\n",
					"    return word_tokenize(text)\r\n",
					"\r\n",
					"# remove stop words \r\n",
					"def remove_stopwords(tokens):\r\n",
					"    stop_words = set(stopwords.words('english'))\r\n",
					"    return [word for word in tokens if word not in stop_words]\r\n",
					"\r\n",
					"# rejoin tokens\r\n",
					"def rejoin_tokens(tokens):\r\n",
					"    return ' '.join(tokens)\r\n",
					"\r\n",
					"# main function for preprocess_text \r\n",
					"def preprocess_text(text):\r\n",
					"    text = to_lowercase(text)\r\n",
					"    text = rm_punctuation(text)\r\n",
					"    tokens = tokenise(text)\r\n",
					"    tokens = remove_stopwords(tokens)\r\n",
					"    return rejoin_tokens(tokens)\r\n",
					"\r\n",
					"def is_string(text):\r\n",
					"    return isinstance(text, str)"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Filter On words counts  \r\n",
					"reddit_df_v1 = (\r\n",
					"    reddit_df.withColumn('wordCount', f.size(f.split(f.col('comment'), ' ')))\r\n",
					"    .filter(f.col('wordCount')>5)\r\n",
					"    .withColumn(\"id\", monotonically_increasing_id())\r\n",
					"    .select('created_utc','id','comment','symbol')\r\n",
					"    ).toPandas()\r\n",
					"\r\n",
					"# Remove content is not english\r\n",
					"reddit_df_v1['is_english'] = reddit_df_v1['comment'].apply(is_english)\r\n",
					"reddit_df_v2 = reddit_df_v1[reddit_df_v1['is_english']]\r\n",
					"\r\n",
					"# data preapration\r\n",
					"reddit_df_v2['preprocessed_text'] = reddit_df_v2['comment'].apply(preprocess_text)\r\n",
					"\r\n",
					"# remove content is not string\r\n",
					"\r\n",
					"reddit_df_v2['is_string']=reddit_df_v2['preprocessed_text'].apply(is_string)\r\n",
					"reddit_df_v3 = reddit_df_v2[reddit_df_v2['is_string']]"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 2. Sentiment analysis with vader"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def analyze_sentiment(text):\r\n",
					"    analyzer = SentimentIntensityAnalyzer()\r\n",
					"    scores = analyzer.polarity_scores(text)\r\n",
					"    return scores['compound']\r\n",
					"\r\n",
					"def categorize_sentiment(compound_score):\r\n",
					"    if compound_score >= 0.05:\r\n",
					"        return 'positive'\r\n",
					"    elif compound_score <= -0.05:\r\n",
					"        return 'negative'\r\n",
					"    else:\r\n",
					"        return 'neutral'"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# get sentiments\r\n",
					"reddit_df_v3['sentiment'] = reddit_df_v3['preprocessed_text'].apply(analyze_sentiment)\r\n",
					"reddit_df_v3['sentiment_category'] = reddit_df_v3['sentiment'].apply(categorize_sentiment)\r\n",
					"\r\n",
					"reddit_df_v4 = reddit_df_v3[['id','created_utc','sentiment','sentiment_category']]\r\n",
					"\r\n",
					"final_df = pd.merge(reddit_df_v1,reddit_df_v4, left_on='id', right_on='id', how='left')\r\n",
					"result = final_df[['created_utc_x','symbol','comment','sentiment','sentiment_category']]"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# write data into datalake\r\n",
					"spdf = spark.createDataFrame(result)\r\n",
					"datalake_nm = 'datalake'+mssparkutils.env.getWorkspaceName()[7:] # get datalake name \r\n",
					"file_path ='abfss://files@{0}.dfs.core.windows.net/synapse/workspaces/data/reddit_sentiments'.format(datalake_nm)\r\n",
					"spdf.write.parquet(file_path,mode='overwrite')"
				],
				"execution_count": 16
			}
		]
	}
}