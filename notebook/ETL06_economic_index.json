{
	"name": "ETL06_economic_index",
	"properties": {
		"folder": {
			"name": "ETL"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4784d7e3-fadc-43cb-abab-6c6227da5438"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 06 Extract economic index from alphavantage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql import functions as f\r\n",
					"import requests\r\n",
					"import pandas as pd \r\n",
					"from datetime import datetime, timedelta\r\n",
					"from notebookutils import mssparkutils"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"# get data from api \n",
					"def get_GDP_PC(apikey):\n",
					"    url = 'https://www.alphavantage.co/query?function=REAL_GDP_PER_CAPITA&apikey={0}'.format(apikey)\n",
					"    r = requests.get(url)\n",
					"    data = r.json()\n",
					"    return data\n",
					"\n",
					"def get_FDI(apikey):\n",
					"    url = 'https://www.alphavantage.co/query?function=FEDERAL_FUNDS_RATE&interval=monthly&apikey={0}'.format(apikey)\n",
					"    r = requests.get(url)\n",
					"    data = r.json()\n",
					"    return data\n",
					"\n",
					"def get_CPI(apikey):\n",
					"    url = 'https://www.alphavantage.co/query?function=CPI&interval=monthly&apikey={0}'.format(apikey)\n",
					"    r = requests.get(url)\n",
					"    data = r.json()\n",
					"    return data\n",
					"\n",
					"def get_INF(apikey):\n",
					"    url = 'https://www.alphavantage.co/query?function=INFLATION&interval=monthly&apikey={0}'.format(apikey)\n",
					"    r = requests.get(url)\n",
					"    data = r.json()\n",
					"    return data\n",
					"\n",
					"def get_UNEM(apikey):\n",
					"    url = 'https://www.alphavantage.co/query?function=UNEMPLOYMENT&apikey={0}'.format(apikey)\n",
					"    r = requests.get(url)\n",
					"    data = r.json()\n",
					"    return data    \n",
					"\n",
					"def transform_spdf(data):\n",
					"    pdf = pd.DataFrame(data)\n",
					"    Spdf = spark.createDataFrame(pdf[['name','data']])\n",
					"    Spdf = (\n",
					"    Spdf.select('name','data')\n",
					"    .withColumn('date',Spdf.data.getItem(\"date\"))\n",
					"    .withColumn('value',Spdf.data.getItem('value'))\n",
					"    .select(\"name\",'date','value')\n",
					"    ) \n",
					"    return Spdf "
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"if __name__ == \"__main__\":\n",
					"    # get apikey from key-vault \n",
					"    apikey = mssparkutils.credentials.getSecret('synapse-key12345','alphavantage-key')\n",
					"\n",
					"    # get data from api\n",
					"    GDP = get_GDP_PC(apikey)\n",
					"    FDI = get_FDI(apikey)\n",
					"    CPI = get_CPI(apikey)\n",
					"    INF = get_INF(apikey)\n",
					"    UNEM = get_UNEM(apikey)\n",
					"    \n",
					"    GDP_SP = transform_spdf(GDP)\n",
					"    GDP_FDI = transform_spdf(FDI)\n",
					"    GDP_CPI = transform_spdf(CPI)\n",
					"    GDP_INF = transform_spdf(INF)\n",
					"    GDP_UNEM = transform_spdf(UNEM)\n",
					"\n",
					"    all_= GDP_SP.union(GDP_FDI).union(GDP_CPI).union(GDP_INF).union(GDP_UNEM)\n",
					"        \n",
					"    print('load data into filepath')\n",
					"    \n",
					"    datalake_nm = 'datalake'+mssparkutils.env.getWorkspaceName()[7:] # get datalake name \n",
					"    file_path ='abfss://files@{0}.dfs.core.windows.net/synapse/workspaces/data/economic_index'.format(datalake_nm) \n",
					"    all_.write.parquet(file_path,mode='overwrite')\n",
					"    spark.stop()"
				],
				"execution_count": 3
			}
		]
	}
}