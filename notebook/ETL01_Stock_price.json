{
	"name": "ETL01_Stock_price",
	"properties": {
		"description": "extract stock price ",
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark001",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0c77139d-76fa-4023-b1d4-73400ae8673f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ac86b402-22ae-4573-b51d-10b49ac53875/resourceGroups/dp000-2gfrzms/providers/Microsoft.Synapse/workspaces/synapse2gfrzms/bigDataPools/spark001",
				"name": "spark001",
				"type": "Spark",
				"endpoint": "https://synapse2gfrzms.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark001",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql import functions as f\r\n",
					"import requests\r\n",
					"import pandas as pd \r\n",
					"import os\r\n",
					"\r\n",
					"# get data from api \r\n",
					"def get_stock_data(symbol,apikey,data_size):\r\n",
					"    url = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={0}&apikey={1}&outputsize={2}'.format(symbol,apikey,data_size)\r\n",
					"    r = requests.get(url)\r\n",
					"    data = r.json()\r\n",
					"    return data\r\n",
					"# read data into pandas df and change schema and add symbol\r\n",
					"def transform_stock_data(data):\r\n",
					"    df = (\r\n",
					"        pd.DataFrame(data['Time Series (Daily)'])\r\n",
					"        .transpose()\r\n",
					"        .reset_index()\r\n",
					"        .rename(columns={\"index\":'date',\"1. open\":'open',\"2. high\":\"high\",\"3. low\":\"low\",\"4. close\":\"close\",\"5. volume\":\"volume\"})\r\n",
					"    )\r\n",
					"    # add symbol\r\n",
					"    df['symbol']=data['Meta Data']['2. Symbol']\r\n",
					"    return df\r\n",
					"\r\n",
					"# join data frame and load into csv\r\n",
					"def concate_df (dataframe_list):\r\n",
					"    united_df = pd.concat(dataframe_list)\r\n",
					"    return united_df\r\n",
					"\r\n",
					"# turn pandas df into spark datafrme and change scheme\r\n",
					"def read_into_sp(dataframe):\r\n",
					"    result = (\r\n",
					"    spark.createDataFrame(dataframe)\r\n",
					"    .withColumn(\"date\", f.to_date(f.col('date'), \"yyyy-MM-dd\"))\r\n",
					"    .withColumn(\"open\",f.col('open').cast(FloatType()))\r\n",
					"    .withColumn(\"high\",f.col('high').cast(FloatType()))\r\n",
					"    .withColumn(\"low\",f.col('low').cast(FloatType()))\r\n",
					"    .withColumn(\"close\",f.col('close').cast(FloatType()))\r\n",
					"    .withColumn(\"volume\",f.col('volume').cast(FloatType()))\r\n",
					"    )\r\n",
					"    return result\r\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"if __name__ == \"__main__\":\r\n",
					"    # adls_path = \"adl://<your-storage-account>.azuredatalakegen2.net/<path-to-folder>/output.parquet\"\r\n",
					"    apikey='YGCUCM7FNF36SZJ0'\r\n",
					"    #data size compact = 100 data points and full data from previous 20 years\r\n",
					"    data_size='full'\r\n",
					"    # set up target ticker \r\n",
					"    symbol_list=['NVDA','AMD','INTC','QCOM','GOOG','MSFT','AMZN','AAPL']\r\n",
					"    # get data from api\r\n",
					"    stock_data = [get_stock_data(symbol,apikey,data_size) for symbol in symbol_list]\r\n",
					"    # transfrom json into data frame\r\n",
					"    transformed_df_list = [transform_stock_data(data) for data in stock_data]\r\n",
					"    final_df = concate_df (transformed_df_list)\r\n",
					"    final_df_sp = read_into_sp(final_df)\r\n",
					"    print('load data into filepath')\r\n",
					"    #add year \r\n",
					"    add_year = final_df_sp.withColumn('year',f.year(f.col('date')))\r\n",
					"    file_path ='abfss://files@datalake2gfrzms.dfs.core.windows.net/synapse/workspaces/synapse2gfrzms/warehouse/Stock_price'\r\n",
					"    add_year.write.parquet(file_path,partitionBy='year',mode='overwrite')"
				],
				"execution_count": null
			}
		]
	}
}