{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import LSTM, Dense\r\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "outputs": [],
      "metadata": {},
      "source": [
        "# read file from datalake\r\n",
        "datalake_nm = 'datalake'+mssparkutils.env.getWorkspaceName()[7:] # get datalake name \r\n",
        "file_path ='abfss://files@{0}.dfs.core.windows.net/synapse/workspaces/data/stock_price'.format(datalake_nm)\r\n",
        "df = spark.read.parquet(file_path )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 01 Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "symbol_list=['NVDA','AMD','INTC','QCOM','GOOG','MSFT','AMZN','AAPL']\r\n",
        "\r\n",
        "# Filter data for NVDA and AAPL\r\n",
        "df = df.withColumn('close',col('close').cast('float')).withColumn('date',to_date(col('date')))\r\n",
        "\r\n",
        "# select data in previous 720 days\r\n",
        "df_list = {symbol:df.filter(col(\"symbol\") == symbol).select(\"date\", \"close\").orderBy(\"date\", ascending=False).limit(720) for symbol in symbol_list}\r\n",
        "\r\n",
        "# Store stock price in dict \r\n",
        "price_list = {}\r\n",
        "for symbol,sdf in df_list.items():\r\n",
        "    pdf = sdf.toPandas().iloc[::-1].reset_index(drop=True)\r\n",
        "    price = pdf['close'].values\r\n",
        "    price_list[symbol]=(price)\r\n",
        "\r\n",
        "# Create scaler for each ticker\r\n",
        "scaler_dict = {}\r\n",
        "\r\n",
        "for symbol in symbol_list:\r\n",
        "    scaler_dict[symbol] = MinMaxScaler()\r\n",
        "\r\n",
        "# Get scaled value and store in dict \r\n",
        "scaled_value_dict = {}\r\n",
        "\r\n",
        "for symbol in symbol_list:\r\n",
        "    scaled_value_dict[symbol] = scaler_dict[symbol].fit_transform(price_list[symbol].reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 02 Training model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create sequences for LSTM\r\n",
        "def create_sequences(data, seq_length):\r\n",
        "    X, y = [], []\r\n",
        "    for i in range(len(data) - seq_length):\r\n",
        "        X.append(data[i:i + seq_length])\r\n",
        "        y.append(data[i + seq_length])\r\n",
        "    return np.array(X), np.array(y)\r\n",
        "\r\n",
        "# Define LSTM model function\r\n",
        "def build_lstm_model(input_shape):\r\n",
        "    model = Sequential([\r\n",
        "        LSTM(50, return_sequences=True, input_shape=(input_shape)),\r\n",
        "        Dropout(0.2),\r\n",
        "        LSTM(50, return_sequences=False),\r\n",
        "        Dropout(0.2),\r\n",
        "        Dense(25),\r\n",
        "        Dense(1)\r\n",
        "    ])\r\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "    return model\r\n",
        "\r\n",
        "seq_length = 30  # Adjust as needed\r\n",
        "\r\n",
        "model_dict = {}\r\n",
        "\r\n",
        "for symbol,scaled_value in scaled_value_dict.items():\r\n",
        "\r\n",
        "    X_, y_ = create_sequences(scaled_value, seq_length)\r\n",
        "    X_= X_.reshape((X_.shape[0], X_.shape[1], 1))\r\n",
        "\r\n",
        "    model = build_lstm_model(input_shape=(X_.shape[1], X_.shape[2]))\r\n",
        "    model.fit(X_, y_, epochs=50, batch_size=32, verbose=1)\r\n",
        "    model_dict[symbol] = model\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Function to predict next n days\r\n",
        "def predict_next_days(model, data, scaler, seq_length, future_days):\r\n",
        "    predicted = []\r\n",
        "    last_sequence = data[-seq_length:]\r\n",
        "    for _ in range(future_days):\r\n",
        "        next_day_prediction = model.predict(last_sequence.reshape(1, seq_length, 1))[0,0]\r\n",
        "        predicted.append(next_day_prediction)\r\n",
        "        last_sequence = np.append(last_sequence[1:], next_day_prediction)\r\n",
        "    return scaler.inverse_transform(np.array(predicted).reshape(-1, 1)).flatten()\r\n",
        "\r\n",
        "# Number of future days to predict\r\n",
        "future_days = 14\r\n",
        "prediction_list = {}\r\n",
        "\r\n",
        "for Ticker in symbol_list:\r\n",
        "\r\n",
        "    predictions = predict_next_days(model_dict[Ticker], scaled_value_dict[Ticker], scaler_dict[Ticker], seq_length, future_days)\r\n",
        "    prediction_list[Ticker] = predictions\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# write data into datalake\r\n",
        "spdf = spark.createDataFrame(pd.DataFrame(prediction_list))\r\n",
        "datalake_nm = 'datalake'+mssparkutils.env.getWorkspaceName()[7:] # get datalake name \r\n",
        "file_path ='abfss://files@{0}.dfs.core.windows.net/synapse/workspaces/data/Stock_predictions'.format(datalake_nm)\r\n",
        "spdf.write.parquet(file_path,mode='overwrite')"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}