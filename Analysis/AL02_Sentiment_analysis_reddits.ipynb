{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Sentiment analysis on reddit comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "!pip install vaderSentiment langdetect emoji nltk vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import udf,monotonically_increasing_id\r\n",
        "from pyspark.sql.types import StringType, FloatType\r\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\r\n",
        "from pyspark.sql import functions as f  \r\n",
        "import requests\r\n",
        "import pandas as pd \r\n",
        "import datetime\r\n",
        "import emoji\r\n",
        "import nltk\r\n",
        "from langdetect import detect\r\n",
        "from langdetect.lang_detect_exception import LangDetectException\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import string\r\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\r\n",
        "from notebookutils import mssparkutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# read file from datalake\r\n",
        "datalake_nm = 'datalake'+mssparkutils.env.getWorkspaceName()[7:] # get datalake name \r\n",
        "file_path ='abfss://files@{0}.dfs.core.windows.net/synapse/workspaces/data/reddit_cms'.format(datalake_nm)\r\n",
        "reddit_df = spark.read.parquet(file_path )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# remove non english word\r\n",
        "def is_english(text):\r\n",
        "    try:\r\n",
        "        return detect(text) == 'en'\r\n",
        "    except LangDetectException:\r\n",
        "        return False\r\n",
        "    \r\n",
        "# change text into lowercase\r\n",
        "def to_lowercase(text):\r\n",
        "    return text.lower()\r\n",
        "\r\n",
        "# remove punctuation\r\n",
        "def rm_punctuation(text):\r\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\r\n",
        "\r\n",
        "def tokenise(text):\r\n",
        "    return word_tokenize(text)\r\n",
        "\r\n",
        "# remove stop words \r\n",
        "def remove_stopwords(tokens):\r\n",
        "    stop_words = set(stopwords.words('english'))\r\n",
        "    return [word for word in tokens if word not in stop_words]\r\n",
        "\r\n",
        "# rejoin tokens\r\n",
        "def rejoin_tokens(tokens):\r\n",
        "    return ' '.join(tokens)\r\n",
        "\r\n",
        "# main function for preprocess_text \r\n",
        "def preprocess_text(text):\r\n",
        "    text = to_lowercase(text)\r\n",
        "    text = rm_punctuation(text)\r\n",
        "    tokens = tokenise(text)\r\n",
        "    tokens = remove_stopwords(tokens)\r\n",
        "    return rejoin_tokens(tokens)\r\n",
        "\r\n",
        "def is_string(text):\r\n",
        "    return isinstance(text, str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Filter On words counts  \r\n",
        "reddit_df_v1 = (\r\n",
        "    reddit_df.withColumn('wordCount', f.size(f.split(f.col('comment'), ' ')))\r\n",
        "    .filter(f.col('wordCount')>5)\r\n",
        "    .withColumn(\"id\", monotonically_increasing_id())\r\n",
        "    .select('created_utc','id','comment','symbol')\r\n",
        "    ).toPandas()\r\n",
        "\r\n",
        "# Remove content is not english\r\n",
        "reddit_df_v1['is_english'] = reddit_df_v1['comment'].apply(is_english)\r\n",
        "reddit_df_v2 = reddit_df_v1[reddit_df_v1['is_english']]\r\n",
        "\r\n",
        "# data preapration\r\n",
        "reddit_df_v2['preprocessed_text'] = reddit_df_v2['comment'].apply(preprocess_text)\r\n",
        "\r\n",
        "# remove content is not string\r\n",
        "\r\n",
        "reddit_df_v2['is_string']=reddit_df_v2['preprocessed_text'].apply(is_string)\r\n",
        "reddit_df_v3 = reddit_df_v2[reddit_df_v2['is_string']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 2. Sentiment analysis with vader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def analyze_sentiment(text):\r\n",
        "    analyzer = SentimentIntensityAnalyzer()\r\n",
        "    scores = analyzer.polarity_scores(text)\r\n",
        "    return scores['compound']\r\n",
        "\r\n",
        "def categorize_sentiment(compound_score):\r\n",
        "    if compound_score >= 0.05:\r\n",
        "        return 'positive'\r\n",
        "    elif compound_score <= -0.05:\r\n",
        "        return 'negative'\r\n",
        "    else:\r\n",
        "        return 'neutral'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# get sentiments\r\n",
        "reddit_df_v3['sentiment'] = reddit_df_v3['preprocessed_text'].apply(analyze_sentiment)\r\n",
        "reddit_df_v3['sentiment_category'] = reddit_df_v3['sentiment'].apply(categorize_sentiment)\r\n",
        "\r\n",
        "reddit_df_v4 = reddit_df_v3[['id','created_utc','sentiment','sentiment_category']]\r\n",
        "\r\n",
        "final_df = pd.merge(reddit_df_v1,reddit_df_v4, left_on='id', right_on='id', how='left')\r\n",
        "result = final_df[['created_utc_x','symbol','comment','sentiment','sentiment_category']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# write data into datalake\r\n",
        "spdf = spark.createDataFrame(result)\r\n",
        "datalake_nm = 'datalake'+mssparkutils.env.getWorkspaceName()[7:] # get datalake name \r\n",
        "file_path ='abfss://files@{0}.dfs.core.windows.net/synapse/workspaces/data/reddit_sentiments'.format(datalake_nm)\r\n",
        "spdf.write.parquet(file_path,mode='overwrite')"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}