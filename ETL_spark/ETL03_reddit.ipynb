{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as f\n",
        "import requests\n",
        "import pandas as pd \n",
        "import datetime"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark001",
              "session_id": "0",
              "statement_id": 3,
              "statement_ids": [
                3
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-06-18T13:22:12.1676829Z",
              "session_start_time": null,
              "execution_start_time": "2024-06-18T13:22:12.3610078Z",
              "execution_finish_time": "2024-06-18T13:22:12.5434797Z",
              "spark_jobs": null,
              "parent_msg_id": "32a8e30e-fccd-43c8-92b0-a4de93132cbe"
            },
            "text/plain": "StatementMeta(spark001, 0, 3, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# import raddit library\n",
        "try:\n",
        "    import praw\n",
        "except:\n",
        "    get_ipython().system('pip install praw')\n",
        "    import praw\n",
        "    \n",
        "from praw.models import MoreComments"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark001",
              "session_id": "0",
              "statement_id": 4,
              "statement_ids": [
                4
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-06-18T13:22:18.1374389Z",
              "session_start_time": null,
              "execution_start_time": "2024-06-18T13:22:18.2944295Z",
              "execution_finish_time": "2024-06-18T13:22:29.1507418Z",
              "spark_jobs": null,
              "parent_msg_id": "5fe00e72-d4ed-4074-be90-8aba81b824af"
            },
            "text/plain": "StatementMeta(spark001, 0, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n  Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting prawcore<3,>=2.1 (from praw)\n  Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\nCollecting update-checker>=0.18 (from praw)\n  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: websocket-client>=0.54.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from praw) (1.7.0)\nRequirement already satisfied: requests<3.0,>=2.6.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from prawcore<3,>=2.1->praw) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.2.2)\nInstalling collected packages: update-checker, prawcore, praw\nSuccessfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# set up The Reddit Instance\n",
        "def set_up_reddit():\n",
        "    user_agent='scraper 1.0 by /u/Adrian_10511'\n",
        "    reddit= praw.Reddit(\n",
        "        client_id='ZSsuFt18m9u6PnXJJwjF1A',\n",
        "        client_secret='cKPHnxi_E9gr7HM5DKdO-XU7nFdO2w',\n",
        "        user_agent = user_agent\n",
        "    )\n",
        "    return reddit\n",
        "    \n",
        "# set up query\n",
        "def search_post(query,reddit,subreddit='stocks',limit_=100):\n",
        "    search_query=query\n",
        "    subreddit_name = subreddit\n",
        "    results = reddit.subreddit(subreddit_name).search(search_query, limit=limit_)\n",
        "    \n",
        "    # change result into dataframe\n",
        "    data=[]\n",
        "    for submission in results:\n",
        "        data.append({\n",
        "            'Title': submission.title,\n",
        "            'Score': submission.score,\n",
        "            'ID': submission.id,\n",
        "            'URL': submission.url,\n",
        "            'Created': submission.created_utc,\n",
        "            'Number of Comments': submission.num_comments\n",
        "        })\n",
        "    post_df = pd.DataFrame(data)\n",
        "    post_df['Created'] = pd.to_datetime(post_df['Created'], unit='s')\n",
        "    return post_df\n",
        "\n",
        "def extract_comment(df,symbol,day=30):\n",
        "    end_date = datetime.datetime.now()\n",
        "    start_date = end_date-datetime.timedelta(days=day)\n",
        "    # Filter the post based on the date range\n",
        "    filtered_df = df[(df['Created'] >= start_date) & (df['Created'] <= end_date)].sort_values(by=['Created'],ascending=False)\n",
        "\n",
        "    # extract df \n",
        "    cmt_list=[]\n",
        "    for post_id in filtered_df['ID']:\n",
        "        for top_level_comment in reddit.submission(post_id).comments:\n",
        "            if isinstance(top_level_comment, MoreComments):\n",
        "                continue\n",
        "            cmt_list.append({\n",
        "                'created_utc':top_level_comment.created_utc,\n",
        "                'comment':top_level_comment.body\n",
        "                })\n",
        "            \n",
        "    # turn into dataframe \n",
        "    cmt_df = pd.DataFrame(cmt_list)\n",
        "    cmt_df['created_utc'] = pd.to_datetime(cmt_df['created_utc'], unit='s')\n",
        "    cmt_df['symbol']= symbol\n",
        "    sort_by_date = cmt_df.sort_values(by=['created_utc'],ascending=False).reset_index(drop=True)\n",
        "    \n",
        "    \n",
        "    return sort_by_date"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark001",
              "session_id": "0",
              "statement_id": 7,
              "statement_ids": [
                7
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-06-18T13:23:30.9909446Z",
              "session_start_time": null,
              "execution_start_time": "2024-06-18T13:23:31.1569572Z",
              "execution_finish_time": "2024-06-18T13:23:31.3388828Z",
              "spark_jobs": null,
              "parent_msg_id": "6dff715c-4bc8-4e70-9151-6effc3932a18"
            },
            "text/plain": "StatementMeta(spark001, 0, 7, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # set up target ticker \n",
        "    symbol_list=['NVDA','AMD','INTC','QCOM','GOOG','MSFT','AMZN','AAPL']\n",
        "    \n",
        "    reddit = set_up_reddit()\n",
        "    list_cm  = [extract_comment(search_post(symbol,reddit),symbol) for symbol in symbol_list]\n",
        "    all_cm_df = pd.concat(list_cm)\n",
        "    all_cm_df['collectedAt']= datetime.datetime.now().date()\n",
        "\n",
        "    spark_df = spark.createDataFrame(all_cm_df)\n",
        "    file_path ='abfss://files@datalake2gfrzms.dfs.core.windows.net/synapse/workspaces/synapse2gfrzms/warehouse/reddit_cms'\n",
        "\n",
        "    spark_df.write.parquet(file_path,partitionBy='collectedAt',mode='overwrite')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark001",
              "session_id": "0",
              "statement_id": 16,
              "statement_ids": [
                16
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-06-18T13:36:51.9902263Z",
              "session_start_time": null,
              "execution_start_time": "2024-06-18T13:36:52.1751071Z",
              "execution_finish_time": "2024-06-18T13:38:01.9120323Z",
              "spark_jobs": null,
              "parent_msg_id": "e91bb7b4-173f-4b16-a398-c647ad624b21"
            },
            "text/plain": "StatementMeta(spark001, 0, 16, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}