{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ETL01_Stock_price"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as f\n",
        "import requests\n",
        "import pandas as pd \n",
        "from notebookutils import mssparkutils\n",
        "\n",
        "# get data from api \n",
        "def get_stock_data(symbol,apikey,data_size):\n",
        "    url = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={0}&apikey={1}&outputsize={2}'.format(symbol,apikey,data_size)\n",
        "    r = requests.get(url)\n",
        "    data = r.json()\n",
        "    return data\n",
        "# read data into pandas df and change schema and add symbol\n",
        "def transform_stock_data(data):\n",
        "    df = (\n",
        "        pd.DataFrame(data['Time Series (Daily)'])\n",
        "        .transpose()\n",
        "        .reset_index()\n",
        "        .rename(columns={\"index\":'date',\"1. open\":'open',\"2. high\":\"high\",\"3. low\":\"low\",\"4. close\":\"close\",\"5. volume\":\"volume\"})\n",
        "    )\n",
        "    # add symbol\n",
        "    df['symbol']=data['Meta Data']['2. Symbol']\n",
        "    return df\n",
        "\n",
        "# join data frame and load into csv\n",
        "def concate_df (dataframe_list):\n",
        "    united_df = pd.concat(dataframe_list)\n",
        "    return united_df\n",
        "\n",
        "# turn pandas df into spark datafrme and change scheme\n",
        "def read_into_sp(dataframe):\n",
        "    result = (\n",
        "    spark.createDataFrame(dataframe)\n",
        "    .withColumn(\"date\", f.to_date(f.col('date'), \"yyyy-MM-dd\"))\n",
        "    .withColumn(\"open\",f.col('open').cast(FloatType()))\n",
        "    .withColumn(\"high\",f.col('high').cast(FloatType()))\n",
        "    .withColumn(\"low\",f.col('low').cast(FloatType()))\n",
        "    .withColumn(\"close\",f.col('close').cast(FloatType()))\n",
        "    .withColumn(\"volume\",f.col('volume').cast(FloatType()))\n",
        "    )\n",
        "    return result"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark001",
              "session_id": "0",
              "statement_id": 2,
              "statement_ids": [
                2
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-06-21T15:16:08.7064894Z",
              "session_start_time": "2024-06-21T15:16:08.7539364Z",
              "execution_start_time": "2024-06-21T15:20:14.5327031Z",
              "execution_finish_time": "2024-06-21T15:20:27.1395671Z",
              "spark_jobs": null,
              "parent_msg_id": "be3dae1f-61ed-4eb2-9e95-81229b0f696a"
            },
            "text/plain": "StatementMeta(spark001, 0, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # get apikey from key-vault \n",
        "    apikey = mssparkutils.credentials.getSecret('synapse-key123','alphavantage-key')\n",
        "    #data size compact = 100 data points and full data from previous 20 years\n",
        "    data_size='full'\n",
        "    # set up target ticker \n",
        "    symbol_list=['NVDA','AMD','INTC','QCOM','GOOG','MSFT','AMZN','AAPL']\n",
        "    # get data from api\n",
        "    stock_data = [get_stock_data(symbol,apikey,data_size) for symbol in symbol_list]\n",
        "    # transfrom json into data frame\n",
        "    transformed_df_list = [transform_stock_data(data) for data in stock_data]\n",
        "    final_df = concate_df (transformed_df_list)\n",
        "    final_df_sp = read_into_sp(final_df)\n",
        "    print('load data into filepath')\n",
        "    #add year column  \n",
        "    add_year = final_df_sp.withColumn('year',f.year(f.col('date')))\n",
        "    file_path ='abfss://files@datalakeqtkzlfx.dfs.core.windows.net/synapse/stock_price'\n",
        "    add_year.write.csv(file_path,mode='overwrite')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark001",
              "session_id": "0",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-06-21T15:24:13.4269095Z",
              "session_start_time": null,
              "execution_start_time": "2024-06-21T15:24:13.5279277Z",
              "execution_finish_time": "2024-06-21T15:24:36.5088729Z",
              "spark_jobs": null,
              "parent_msg_id": "3a55b8e5-1a41-4382-a209-11784f7cfeea"
            },
            "text/plain": "StatementMeta(spark001, 0, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load data into filepath\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
